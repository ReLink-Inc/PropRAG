import json
import os
import logging
from dataclasses import dataclass, field, asdict
from datetime import datetime
from typing import Union, Optional, List, Set, Dict, Any, Tuple, Literal
import numpy as np
import importlib
from collections import defaultdict
from transformers import HfArgumentParser
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from igraph import Graph
import igraph as ig
import numpy as np
from collections import defaultdict
import re
import time

from .llm import _get_llm_class, BaseLLM
from .embedding_model import _get_embedding_model_class, BaseEmbeddingModel
from .embedding_store import EmbeddingStore
from .information_extraction import OpenIE
from .information_extraction.openie_vllm_offline import VLLMOfflineOpenIE
from .information_extraction.enhanced_openie import EnhancedOpenIE
from .evaluation.retrieval_eval import RetrievalRecall
from .evaluation.qa_eval import QAExactMatch, QAF1Score
from .prompts.linking import get_query_instruction
from .prompts.prompt_template_manager import PromptTemplateManager
from .utils.misc_utils import *
from .utils.embed_utils import retrieve_knn
from .utils.typing import Proposition
from .utils.config_utils import BaseConfig
from .graph_beam_search import BeamSearchPathFinder

logger = logging.getLogger(__name__)

class PropRAG:

    def __init__(self, global_config=None, save_dir=None, llm_model_name=None, embedding_model_name=None, llm_base_url=None):
        """
        Initializes an instance of the class and its related components.

        Attributes:
            global_config (BaseConfig): The global configuration settings for the instance. An instance
                of BaseConfig is used if no value is provided.
            saving_dir (str): The directory where specific PropRAG instances will be stored. This defaults
                to `outputs` if no value is provided.
            llm_model (BaseLLM): The language model used for processing based on the global
                configuration settings.
            openie (Union[OpenIE, VLLMOfflineOpenIE]): The Open Information Extraction module
                configured in either online or offline mode based on the global settings.
            graph: The graph instance initialized by the `initialize_graph` method.
            embedding_model (BaseEmbeddingModel): The embedding model associated with the current
                configuration.
            chunk_embedding_store (EmbeddingStore): The embedding store handling chunk embeddings.
            entity_embedding_store (EmbeddingStore): The embedding store handling entity embeddings.
            proposition_embedding_store (EmbeddingStore): The embedding store handling proposition embeddings.
            prompt_template_manager (PromptTemplateManager): The manager for handling prompt templates
                and roles mappings.
            openie_results_path (str): The file path for storing Open Information Extraction results
                based on the dataset and LLM name in the global configuration.
            ready_to_retrieve (bool): A flag indicating whether the system is ready for retrieval
                operations.

        Parameters:
            global_config: The global configuration object. Defaults to None, leading to initialization
                of a new BaseConfig object.
            working_dir: The directory for storing working files. Defaults to None, constructing a default
                directory based on the class name and timestamp.
            llm_model_name: LLM model name, can be inserted directly as well as through configuration file.
            embedding_model_name: Embedding model name, can be inserted directly as well as through configuration file.
            llm_base_url: LLM URL for a deployed vLLM model, can be inserted directly as well as through configuration file.
        """
        if global_config is None:
            self.global_config = BaseConfig()
            print("BaseConfig!")
        else:
            self.global_config = global_config
            print("GlobalConfig!")
        print("global_config: ", self.global_config)
        #Overwriting Configuration if Specified
        if save_dir is not None:
            self.global_config.save_dir = save_dir

        if llm_model_name is not None:
            self.global_config.llm_name = llm_model_name

        if embedding_model_name is not None:
            self.global_config.embedding_model_name = embedding_model_name

        if llm_base_url is not None:
            self.global_config.llm_base_url = llm_base_url

        _print_config = ",\n  ".join([f"{k} = {v}" for k, v in asdict(self.global_config).items()])
        logger.debug(f"PropRAG init with config:\n  {_print_config}\n")

        #LLM and embedding model specific working directories are created under every specified saving directories
        llm_label = self.global_config.llm_name.replace("/", "_")
        embedding_label = self.global_config.embedding_model_name.replace("/", "_")
        self.working_dir = os.path.join(self.global_config.save_dir, f"{llm_label}_{embedding_label}")

        if not os.path.exists(self.working_dir):
            logger.info(f"Creating working directory: {self.working_dir}")
            os.makedirs(self.working_dir, exist_ok=True)

        self.llm_model: BaseLLM = _get_llm_class(self.global_config)

        self.openie = EnhancedOpenIE(llm_model=self.llm_model)

        self.graph = self.initialize_graph()

        if self.global_config.openie_mode == 'offline':
            self.embedding_model = None
        else:
            self.embedding_model: BaseEmbeddingModel = _get_embedding_model_class(self.global_config)
        self.chunk_embedding_store = EmbeddingStore(self.embedding_model,
                                                    os.path.join(self.working_dir, "chunk_embeddings"),
                                                    self.global_config.embedding_batch_size, 'chunk')
        self.entity_embedding_store = EmbeddingStore(self.embedding_model,
                                                     os.path.join(self.working_dir, "entity_embeddings"),
                                                     self.global_config.embedding_batch_size, 'entity')
        self.proposition_embedding_store = EmbeddingStore(self.embedding_model,
                                                   os.path.join(self.working_dir, "proposition_embeddings"),
                                                   self.global_config.embedding_batch_size, 'proposition')

        self.prompt_template_manager = PromptTemplateManager(role_mapping={"system": "system", "user": "user", "assistant": "assistant"})

        self.openie_results_path = os.path.join(self.global_config.save_dir,f'openie_results_ner_{self.global_config.llm_name.replace("/", "_")}.json')

        self.ready_to_retrieve = False


    def initialize_graph(self):
        """
        Initializes a graph using a GraphML file if available or creates a new graph.

        The function attempts to load a pre-existing graph stored in a GraphML file. If the file
        is not present or the graph needs to be created from scratch, it initializes a new directed
        or undirected graph based on the global configuration. If the graph is loaded successfully
        from the file, pertinent information about the graph (number of nodes and edges) is logged.

        Returns:
            ig.Graph: A pre-loaded or newly initialized graph.

        Raises:
            None
        """
        self._graphml_xml_file = os.path.join(
            self.working_dir, f"graph_{self.global_config.synonymy_edge_sim_threshold}.graphml"
        )

        preloaded_graph = None

        if not self.global_config.force_index_from_scratch:
            if os.path.exists(self._graphml_xml_file):
                preloaded_graph = ig.Graph.Read_GraphML(self._graphml_xml_file)

        if preloaded_graph is None:
            return ig.Graph(directed=self.global_config.is_directed_graph)
        else:
            logger.info(
                f"Loaded graph from {self._graphml_xml_file} with {preloaded_graph.vcount()} nodes, {preloaded_graph.ecount()} edges"
            )
            return preloaded_graph

    def pre_openie(self,  docs: List[str]):
        logger.info(f"Indexing Documents")
        logger.info(f"Performing OpenIE Offline")

        chunks = self.chunk_embedding_store.get_missing_string_hash_ids(docs)

        all_openie_info, chunk_keys_to_process = self.load_existing_openie(chunks.keys())
        new_openie_rows = {k : chunks[k] for k in chunk_keys_to_process}

        if len(chunk_keys_to_process) > 0:
            # Enhanced OpenIE returns proposition results as well
            logger.info("Running EnhancedOpenIE in pre_openie")
            new_ner_results_dict, new_proposition_results_dict = self.openie.batch_openie(
                new_openie_rows)
            self.merge_openie_results(
                all_openie_info, 
                new_openie_rows, 
                new_ner_results_dict, 
                new_proposition_results_dict
            )
            
        if self.global_config.save_openie:
            self.save_openie_results(all_openie_info)

        assert False, logger.info('Done with OpenIE, run online indexing for future retrieval.')

    def index(self, docs: List[str]):
        """
        Indexes the given documents based on the HippoRAG 2 framework which generates an OpenIE knowledge graph
        based on the given documents and encodes passages, entities and propositions separately for later retrieval.

        Parameters:
            docs : List[str]
                A list of documents to be indexed.
        """

        logger.info(f"Indexing Documents")

        logger.info(f"Performing OpenIE")

        if self.global_config.openie_mode == 'offline':
            self.pre_openie(docs)

        self.chunk_embedding_store.insert_strings(docs)
        chunks = self.chunk_embedding_store.get_text_for_all_rows()

        all_openie_info, chunk_keys_to_process = self.load_existing_openie(chunks.keys())
        new_openie_rows = {k : chunks[k] for k in chunk_keys_to_process}

        if len(chunk_keys_to_process) > 0:
            # Check if we're using the enhanced OpenIE with propositions
            logger.info("Running EnhancedOpenIE in index")
            new_ner_results_dict, new_proposition_results_dict = self.openie.batch_openie(
                new_openie_rows)
            self.merge_openie_results(
                all_openie_info, 
                new_openie_rows, 
                new_ner_results_dict, 
                new_proposition_results_dict
            )

        if self.global_config.save_openie:
            self.save_openie_results(all_openie_info)

        ner_results_dict, _ = reformat_openie_results(all_openie_info)
        print(len(chunks), len(ner_results_dict), len(all_openie_info))
        assert len(chunks) == len(ner_results_dict)

        # prepare data_store
        chunk_ids = list(chunks.keys())
        # Check if we are using propositions
        logger.info(f"Processing propositions for embedding")
        # Get propositions from openie results
        proposition_results_dict = {}
        
        # Initialize proposition to passages map
        self.proposition_to_passages = defaultdict(set)
        
        for chunk_item in all_openie_info:
            chunk_id = chunk_item['idx']
            if 'propositions' in chunk_item:
                proposition_results_dict[chunk_id] = PropositionRawOutput(
                    chunk_id=chunk_id,
                    response="",
                    metadata={},
                    propositions=chunk_item['propositions']
                )
                
                # Directly map each proposition to its containing passage (chunk)
                for prop in chunk_item['propositions']:
                    if "text" in prop:
                        prop_text = prop["text"]
                        prop_key = compute_mdhash_id(prop_text, prefix="proposition-")
                        # if prop_key in self.proposition_to_passages:
                        #     print('prop_key: ', prop_key)
                        #     print('passage: ', chunk_id)
                        #     print('self.proposition_to_passages[prop_key]: ', self.proposition_to_passages[prop_key])
                        self.proposition_to_passages[prop_key].add(chunk_id)
        
        # Extract entities from propositions
        chunk_propositions = [proposition_results_dict[chunk_id].propositions for chunk_id in chunk_ids]
        self.chunk_propositions = {chunk_id: chunk_propositions[i] for i, chunk_id in enumerate(chunk_ids)}
        entity_nodes, chunk_proposition_entities = extract_proposition_entities(chunk_propositions)
        propositions = flatten_propositions(chunk_propositions)

        logger.info(f"Encoding Entities")
        self.entity_embedding_store.insert_strings(entity_nodes)
        
        logger.info(f"Encoding Propositions")
        self.proposition_embedding_store.insert_strings([prop['text'] for prop in propositions])
        
        # Store mapping of propositions to entities for graph construction
        self.proposition_to_entities_map = {}
        for prop in propositions:
            # Ensure prop has both text and entities
            if "text" in prop and "entities" in prop:
                prop_text = prop["text"]
                prop_key = compute_mdhash_id(prop_text, prefix="proposition-")
                self.proposition_to_entities_map[prop_key] = prop["entities"]
            else:
                logger.warning(f"Skipping proposition without required fields: {prop}")
        
        # Log statistics about the proposition to passage mapping
        props_with_passages = sum(len(self.proposition_to_passages[prop]) for prop in self.proposition_to_passages if self.proposition_to_passages[prop])
        total_props = len(self.proposition_to_entities_map)
        avg_passages = sum(len(passages) for passages in self.proposition_to_passages.values()) / max(1, len(self.proposition_to_passages))
        
        logger.info(f"Built proposition-passage map: {props_with_passages}/{total_props} propositions mapped.")
        logger.info(f"Average passages per proposition: {avg_passages:.2f}")


        logger.info(f"Constructing Graph")

        self.node_to_node_stats = {}
        self.ent_node_to_num_chunk = {}

        # Store all_openie_info for later use
        self.openie_info = all_openie_info
        
        logger.info("Using proposition-based graph construction")
        self.add_proposition_edges_with_entity_connections()
        num_new_chunks = self.add_passage_edges(chunk_ids, chunk_proposition_entities)

        if num_new_chunks > 0:
            logger.info(f"Found {num_new_chunks} new chunks to save into graph.")
            self.add_synonymy_edges()

            self.augment_graph()
            self.save_igraph()

    def retrieve(self,
                 queries: List[str],
                 num_to_retrieve: int = None,
                 gold_docs: List[List[str]] = None,
                 use_beam_search: bool = False,
                 beam_width: int = 5,
                 embedding_combination: str = "concatenate",
                 second_stage_filter_k: int = 0,
                 sim_threshold: float = 0.75) -> List[QuerySolution] | Tuple[List[QuerySolution], Dict]:
        """
        Performs retrieval using the PropRAG framework, which consists of several steps:
        - Proposition Retrieval
        - Dense passage scoring
        - Personalized PageRank based re-ranking

        Parameters:
            queries: List[str]
                A list of query strings for which documents are to be retrieved.
            num_to_retrieve: int, optional
                The maximum number of documents to retrieve for each query. If not specified, defaults to
                the `retrieval_top_k` value defined in the global configuration.
            gold_docs: List[List[str]], optional
                A list of lists containing gold-standard documents corresponding to each query. Required
                if retrieval performance evaluation is enabled (`do_eval_retrieval` in global configuration).
            use_beam_search: bool, optional
                If True, uses beam search for graph traversal to find relevant documents.
            beam_width: int, optional
                The number of paths to consider in beam search. Only used if use_beam_search is True.
            embedding_combination: str, optional
                Method to combine proposition embeddings in beam search paths. Options:
                - "concatenate": Re-embed the concatenated text of propositions (default)
                - "average": Use the average of embeddings
                - "weighted_average": Weight by position in path
                - "max_pool": Use element-wise maximum values
                - "attention": Weight by attention to query
                - "predictor": Use a trained model to predict combined embedding (if available)
            second_stage_filter_k: int, optional
                If > 0, apply a second stage filtering using concatenate method on top K candidates
                during beam search. Set to 0 to disable (default).
            sim_threshold: float, optional
                Threshold for considering edges as synonym connections (default: 0.75).

        Returns:
            List[QuerySolution] or (List[QuerySolution], Dict)
                If retrieval performance evaluation is not enabled, returns a list of QuerySolution objects, each containing
                the retrieved documents and their scores for the corresponding query. If evaluation is enabled, also returns
                a dictionary containing the evaluation metrics computed over the retrieved results.
        """

        if num_to_retrieve is None:
            num_to_retrieve = self.global_config.retrieval_top_k

        if gold_docs is not None:
            retrieval_recall_evaluator = RetrievalRecall(global_config=self.global_config)

        if not self.ready_to_retrieve:
            self.prepare_retrieval_objects()

        self.get_query_embeddings(queries)
        # log_file = open("retrieval_log_3.jsonl", "w")
        retrieval_results = []
        for q_idx, query in tqdm(enumerate(queries), desc="Retrieving", total=len(queries)):
            sorted_doc_ids, sorted_doc_scores, paths, top_entities_and_scores, other_candidates = self.graph_search_with_proposition_entities(query=query,
                                                                                        link_top_k=self.global_config.linking_top_k,
                                                                                        passage_node_weight=self.global_config.passage_node_weight, 
                                                                                        use_focused_graph=True, 
                                                                                        focus_top_k=100)
            

            top_k_docs = [self.chunk_embedding_store.get_row(self.passage_node_keys[idx])["content"] for idx in sorted_doc_ids[:num_to_retrieve]]

            retrieval_results.append(QuerySolution(question=query, docs=top_k_docs, doc_scores=sorted_doc_scores[:num_to_retrieve]))
        # Evaluate retrieval
        if gold_docs is not None:
            k_list = [1, 2, 3, 4, 5, 10, 20, 30, 50, 100, 150, 200]
            overall_retrieval_result, example_retrieval_results = retrieval_recall_evaluator.calculate_metric_scores(gold_docs=gold_docs, retrieved_docs=[retrieval_result.docs for retrieval_result in retrieval_results], k_list=k_list)
            logger.info(f"Evaluation results for retrieval: {overall_retrieval_result}")

            return retrieval_results, overall_retrieval_result
        else:
            return retrieval_results

    def rag_qa(self,
               queries: List[str|QuerySolution],
               gold_docs: List[List[str]] = None,
               gold_answers: List[List[str]] = None,
               use_beam_search: bool = False,
               beam_width: int = 5,
               embedding_combination: str = "concatenate",
               second_stage_filter_k: int = 0,
               sim_threshold: float = 0.75) -> Tuple[List[QuerySolution], List[str], List[Dict]] | Tuple[List[QuerySolution], List[str], List[Dict], Dict, Dict]:
        """
        Performs retrieval-augmented generation enhanced QA using the HippoRAG 2 framework.

        This method can handle both string-based queries and pre-processed QuerySolution objects. Depending
        on its inputs, it returns answers only or additionally evaluate retrieval and answer quality using
        recall @ k, exact match and F1 score metrics.

        Parameters:
            queries (List[Union[str, QuerySolution]]): A list of queries, which can be either strings or
                QuerySolution instances. If they are strings, retrieval will be performed.
            gold_docs (Optional[List[List[str]]]): A list of lists containing gold-standard documents for
                each query. This is used if document-level evaluation is to be performed. Default is None.
            gold_answers (Optional[List[List[str]]]): A list of lists containing gold-standard answers for
                each query. Required if evaluation of question answering (QA) answers is enabled. Default
                is None.
            use_beam_search (bool, optional): If True, uses beam search for proposition path finding
                during retrieval.
            beam_width (int, optional): The number of paths to consider in beam search when use_beam_search
                is True. Default is 5.
            embedding_combination (str, optional): Method to combine proposition embeddings in beam search:
                - "concatenate": Re-embed the concatenated text (default)
                - "average": Use the average of embeddings
                - "weighted_average": Weight by position in path
                - "max_pool": Use element-wise maximum values
                - "attention": Weight by attention to query
                - "predictor": Use a trained model to predict combined embedding
            second_stage_filter_k (int, optional): If > 0, apply a second stage filtering with concatenate
                method on top K candidates during beam search.
            sim_threshold (float, optional): Threshold for considering edges as synonyms during beam search
                (default: 0.75).

        Returns:
            Union[
                Tuple[List[QuerySolution], List[str], List[Dict]],
                Tuple[List[QuerySolution], List[str], List[Dict], Dict, Dict]
            ]: A tuple that always includes:
                - List of QuerySolution objects containing answers and metadata for each query.
                - List of response messages for the provided queries.
                - List of metadata dictionaries for each query.
                If evaluation is enabled, the tuple also includes:
                - A dictionary with overall results from the retrieval phase (if applicable).
                - A dictionary with overall QA evaluation metrics (exact match and F1 scores).

        """
        logger.info(f"Starting RAG QA with {len(queries)} queries")
        
        if gold_answers is not None:
            qa_em_evaluator = QAExactMatch(global_config=self.global_config)
            qa_f1_evaluator = QAF1Score(global_config=self.global_config)

        # Retrieving (if necessary)
        overall_retrieval_result = None

        # try:
        if not isinstance(queries[0], QuerySolution):
            logger.info("Performing retrieval for raw query strings")
            # try:
            if gold_docs is not None:
                logger.info("Retrieving with gold docs for evaluation")
                queries, overall_retrieval_result = self.retrieve(
                    queries=queries, 
                    gold_docs=gold_docs, 
                    use_beam_search=use_beam_search, 
                    beam_width=beam_width,
                    embedding_combination=embedding_combination,
                    second_stage_filter_k=second_stage_filter_k,
                    sim_threshold=sim_threshold
                )
            else:
                logger.info("Retrieving without gold docs")
                queries = self.retrieve(
                    queries=queries, 
                    use_beam_search=use_beam_search, 
                    beam_width=beam_width,
                    embedding_combination=embedding_combination,
                    second_stage_filter_k=second_stage_filter_k,
                    sim_threshold=sim_threshold
                )
            logger.info(f"Retrieval completed, got {len(queries)} results")

        # Performing QA
        logger.info("Performing QA on retrieved documents")
        queries_solutions, all_response_message, all_metadata = self.qa(queries)

        # Evaluating QA
        if gold_answers is not None:
            logger.info("Evaluating QA results against gold answers")
            overall_qa_em_result, example_qa_em_results = qa_em_evaluator.calculate_metric_scores(
                gold_answers=gold_answers, predicted_answers=[qa_result.answer for qa_result in queries_solutions],
                aggregation_fn=np.max)
            overall_qa_f1_result, example_qa_f1_results = qa_f1_evaluator.calculate_metric_scores(
                gold_answers=gold_answers, predicted_answers=[qa_result.answer for qa_result in queries_solutions],
                aggregation_fn=np.max)

            # round off to 4 decimal places for QA results
            overall_qa_em_result.update(overall_qa_f1_result)
            overall_qa_results = overall_qa_em_result
            overall_qa_results = {k: round(float(v), 4) for k, v in overall_qa_results.items()}
            logger.info(f"Evaluation results for QA: {overall_qa_results}")

            # Save retrieval and QA results
            for idx, q in enumerate(queries_solutions):
                q.gold_answers = list(gold_answers[idx])
                if gold_docs is not None:
                    q.gold_docs = gold_docs[idx]

            return queries_solutions, all_response_message, all_metadata, overall_retrieval_result, overall_qa_results
        else:
            logger.info("Completed RAG QA without evaluation")
            return queries_solutions, all_response_message, all_metadata


    def qa(self, queries: List['QuerySolution']) -> Tuple[List['QuerySolution'], List[str], List[Dict]]:
        """
        Executes question-answering (QA) inference using a provided set of query solutions and a language model,
        leveraging multithreading for faster inference.

        Parameters:
            queries: List[QuerySolution]
                A list of QuerySolution objects that contain the user queries, retrieved documents, and other related information.

        Returns:
            Tuple[List[QuerySolution], List[str], List[Dict]]
                A tuple containing:
                - A list of updated QuerySolution objects with the predicted answers embedded in them.
                - A list of raw response messages from the language model.
                - A list of metadata dictionaries associated with the results.
        """
        # --- Step 1: Prepare all prompts (remains sequential, usually fast) ---
        all_qa_messages = []
        for query_solution in tqdm(queries, desc="Collecting QA prompts"):
            retrieved_passages = query_solution.docs[:self.global_config.qa_top_k]

            # Build the user prompt content
            prompt_user_content = ""
            for passage in retrieved_passages:
                 # Ensure passage is treated as string or has appropriate __str__
                prompt_user_content += f'Wikipedia Title: {str(passage)}\n\n' # Added str() just in case
            prompt_user_content += 'Question: ' + query_solution.question + '\nThought: '

            # Determine the correct prompt template name
            template_name = f'rag_qa_{self.global_config.dataset}'
            if not self.prompt_template_manager.is_template_name_valid(name=template_name):
                logger.debug(
                    f"{template_name} does not have a customized prompt template. Using MUSIQUE's prompt template instead."
                )
                template_name = 'rag_qa_musique' # Assuming 'musique' is the fallback

            # Render the full prompt message structure (e.g., system/user messages)
            # Assuming render returns the structure needed by self.llm_model.infer
            rendered_message = self.prompt_template_manager.render(
                name=template_name,
                prompt_user=prompt_user_content # Pass the content, render wraps it
            )
            all_qa_messages.append(rendered_message)

        # --- Step 2: Run inference in parallel using ThreadPoolExecutor ---
        all_qa_results = [None] * len(all_qa_messages)
        logger.info(f"Starting QA inference.")

        def response_checker(response_message, finish_reason, params):
            if finish_reason == "length":
                logger.warning(f"Response for query {response_message} finishes early due to length. Retrying with temperature += 0.1...")
                params['temperature'] += 0.1
                return False
            if 'Answer:' not in response_message:
                logger.warning(f"Response for query {response_message} does not contain 'Answer:'. Retrying with temperature += 0.1...")
                params['temperature'] += 0.1
                return False
            return True

        # Use ThreadPoolExecutor to run self.llm_model.infer concurrently
        with ThreadPoolExecutor(max_workers=300) as executor:
            # Create an iterator for the results using executor.map
            # executor.map ensures results are returned in the order submissions were made
            # results_iterator = executor.map(self.llm_model.infer, all_qa_messages)
            qa_futures = {}
            for i, qa_message in enumerate(all_qa_messages):
                future = executor.submit(self.llm_model.infer, qa_message, response_checker=response_checker, max_completion_tokens=2048)
                qa_futures[future] = i


            with RetryExecutor(executor, qa_futures,
                             lambda i: ((self.llm_model.infer, all_qa_messages[i]), {"response_checker": response_checker}),
                             desc="QA Reading (Parallel)") as retry_exec:
                def process_qa(future, i, pbar):
                    result = future.result()
                    all_qa_results[i] = result
                retry_exec.process(process_qa)

        logger.info("Finished QA inference.")

        # --- Step 3: Unpack results (remains the same) ---
        # Check if results were actually returned before unpacking
        if not all_qa_results:
             logger.warning("No QA results were returned from parallel inference.")
             # Handle this case appropriately, maybe return empty lists or raise error
             return [], [], []

        # Ensure all results are tuples of expected length before unpacking
        # (Optional but safer if infer could potentially return something unexpected on error)
        valid_results = [res for res in all_qa_results if isinstance(res, tuple) and len(res) == 3]
        if len(valid_results) != len(all_qa_results):
            logger.warning(f"Expected 3 elements per result, but got inconsistencies. Found {len(valid_results)} valid results out of {len(all_qa_results)}.")
            # Decide how to handle partial failures - here we proceed with valid ones,
            # but indices might mismatch if not careful. A more robust approach might be needed.
            # For simplicity now, we assume all results are valid if any exist.

        all_response_message, all_metadata, all_cache_hit = zip(*all_qa_results) # Unpack results
        all_response_message, all_metadata = list(all_response_message), list(all_metadata)

        # --- Step 4: Process responses (remains sequential, usually fast) ---
        queries_solutions = []
        for query_solution_idx, query_solution in tqdm(enumerate(queries), desc="Extracting Answers from LLM Response"):
            # Make sure we have a response for this index
            if query_solution_idx < len(all_response_message):
                response_content = all_response_message[query_solution_idx]
                pred_ans = response_content # Default to full response
                # print("--------------------------------")
                # print("query: ", query_solution.question)
                # print("query_solution_idx: ", query_solution_idx)
                # print("response_content: ", response_content)
                if response_content: # Check if response is not None or empty
                    try:
                        # Attempt to split only if 'Answer:' is present
                        if 'Answer:' in response_content:
                            pred_ans = response_content.split('Answer:', 1)[1].strip()
                        else:
                            logger.warning(f"Response for query {query_solution_idx} does not contain 'Answer:'. Using full response. Original response: {response_content}")
                            pred_ans = response_content # Use full response if marker not found
                    except Exception as e:
                        logger.warning(f"Error parsing answer for query {query_solution_idx} from response: '{response_content}'. Error: {str(e)}")
                        pred_ans = response_content # Fallback to full response on error
                else:
                     logger.warning(f"Received empty or None response for query {query_solution_idx}.")
                     pred_ans = "[No Answer Received]" # Assign a placeholder

                query_solution.answer = pred_ans
                queries_solutions.append(query_solution)
            else:
                 logger.error(f"Mismatch in number of queries and responses. Missing response for query index {query_solution_idx}.")
                 # Optionally append a modified query_solution indicating failure
                 # query_solution.answer = "[Error: Missing LLM Response]"
                 # queries_solutions.append(query_solution)


        return queries_solutions, all_response_message, all_metadata


    def add_passage_edges(self, chunk_ids: List[str], chunk_proposition_entities: List[List[str]]):
        """
        Adds edges connecting passage nodes to phrase nodes in the graph.

        This method is responsible for iterating through a list of chunk identifiers
        and their corresponding proposition entities. It calculates and adds new edges
        between the passage nodes (defined by the chunk identifiers) and the phrase
        nodes (defined by the computed unique hash IDs of proposition entities). The method
        also updates the node-to-node statistics map and keeps count of newly added
        passage nodes.

        Parameters:
            chunk_ids : List[str]
                A list of identifiers representing passage nodes in the graph.
            chunk_proposition_entities : List[List[str]]
                A list of lists where each sublist contains entities (strings) associated
                with the corresponding chunk in the chunk_ids list.

        Returns:
            int
                The number of new passage nodes added to the graph.
        """

        if "name" in self.graph.vs.attribute_names():
            current_graph_nodes = set(self.graph.vs["name"])
        else:
            current_graph_nodes = set()

        num_new_chunks = 0

        logger.info(f"Connecting passage nodes to phrase nodes.")

        for idx, chunk_key in tqdm(enumerate(chunk_ids)):

            if chunk_key not in current_graph_nodes:
                # if "Birger Furugård" in chunk_key:
                #     logger.info(f"Chunk key: {chunk_key}")
                for chunk_ent in chunk_proposition_entities[idx]:
                    # node_key = compute_mdhash_id(chunk_ent, prefix="proposition-")
                    # if "Birger Furugård" in chunk_key:
                    #     logger.info(f"Chunk entity: {chunk_ent}")
                    node_key = compute_mdhash_id(chunk_ent, prefix="entity-")

                    self.node_to_node_stats[(chunk_key, node_key)] = 1.0

                num_new_chunks += 1

        return num_new_chunks

    def add_synonymy_edges(self):
        """
        Adds synonymy edges between similar nodes in the graph to enhance connectivity by identifying and linking synonym entities.

        This method performs key operations to compute and add synonymy edges. It first retrieves embeddings for all nodes, then conducts
        a nearest neighbor (KNN) search to find similar nodes. These similar nodes are identified based on a score threshold, and edges
        are added to represent the synonym relationship.

        Attributes:
            entity_id_to_row: dict (populated within the function). Maps each entity ID to its corresponding row data, where rows
                              contain `content` of entities used for comparison.
            entity_embedding_store: Manages retrieval of texts and embeddings for all rows related to entities.
            global_config: Configuration object that defines parameters such as `synonymy_edge_topk`, `synonymy_edge_sim_threshold`,
                           `synonymy_edge_query_batch_size`, and `synonymy_edge_key_batch_size`.
            node_to_node_stats: dict. Stores scores for edges between nodes representing their relationship.

        """
        logger.info(f"Expanding graph with synonymy edges")

        self.entity_id_to_row = self.entity_embedding_store.get_text_for_all_rows()
        entity_node_keys = list(self.entity_id_to_row.keys())

        logger.info(f"Performing KNN retrieval for each phrase nodes ({len(entity_node_keys)}).")

        entity_embs = self.entity_embedding_store.get_embeddings(entity_node_keys)

        # Here we build synonymy edges only between newly inserted phrase nodes and all phrase nodes in the storage to reduce cost for incremental graph updates
        query_node_key2knn_node_keys = retrieve_knn(query_ids=entity_node_keys,
                                                    key_ids=entity_node_keys,
                                                    query_vecs=entity_embs,
                                                    key_vecs=entity_embs,
                                                    k=self.global_config.synonymy_edge_topk,
                                                    query_batch_size=self.global_config.synonymy_edge_query_batch_size,
                                                    key_batch_size=self.global_config.synonymy_edge_key_batch_size,
                                                    threshold_score=self.global_config.synonymy_edge_sim_threshold)

        num_synonym_triple = 0
        synonym_candidates = []  # [(node key, [(synonym node key, corresponding score), ...]), ...]

        for node_key in tqdm(query_node_key2knn_node_keys.keys(), total=len(query_node_key2knn_node_keys)):
            synonyms = []

            entity = self.entity_id_to_row[node_key]["content"]

            if len(re.sub('[^A-Za-z0-9]', '', entity)) > 2:
                nns = query_node_key2knn_node_keys[node_key]

                num_nns = 0
                for nn, score in zip(nns[0], nns[1]):
                    # print("node_key", node_key)
                    # print("nn", nn)
                    # print("score", score)
                    if score < self.global_config.synonymy_edge_sim_threshold or num_nns > 100:
                        break

                    nn_phrase = self.entity_id_to_row[nn]["content"]

                    if nn != node_key and nn_phrase != '':
                        # print("adding")
                        sim_edge = (node_key, nn)
                        synonyms.append((nn, score))
                        num_synonym_triple += 1

                        self.node_to_node_stats[sim_edge] = self.node_to_node_stats.get(sim_edge, 0) + score
                        num_nns += 1
                    # else:
                        # print("not adding")

            synonym_candidates.append((node_key, synonyms))
        # print("synonym_candidates", synonym_candidates)

    def load_existing_openie(self, chunk_keys: List[str]) -> Tuple[List[dict], Set[str]]:
        """
        Loads existing OpenIE results from the specified file if it exists and combines
        them with new content while standardizing indices. If the file does not exist or
        is configured to be re-initialized from scratch with the flag `force_openie_from_scratch`,
        it prepares new entries for processing.

        Args:
            chunk_keys (List[str]): A list of chunk keys that represent identifiers
                                     for the content to be processed.

        Returns:
            Tuple[List[dict], Set[str]]: A tuple where the first element is the existing OpenIE
                                         information (if any) loaded from the file, and the
                                         second element is a set of chunk keys that still need to
                                         be saved or processed.
        """

        # combine openie_results with contents already in file, if file exists
        chunk_keys_to_save = set()

        if not self.global_config.force_openie_from_scratch and os.path.isfile(self.openie_results_path):
            openie_results = json.load(open(self.openie_results_path))
            all_openie_info = openie_results.get('docs', [])

            #Standardizing indices for OpenIE Files.

            renamed_openie_info = []
            for openie_info in all_openie_info:
                openie_info['idx'] = compute_mdhash_id(openie_info['passage'], 'chunk-')
                renamed_openie_info.append(openie_info)

            all_openie_info = renamed_openie_info

            existing_openie_keys = set([info['idx'] for info in all_openie_info])

            for chunk_key in chunk_keys:
                if chunk_key not in existing_openie_keys:
                    chunk_keys_to_save.add(chunk_key)
        else:
            all_openie_info = []
            chunk_keys_to_save = chunk_keys

        return all_openie_info, chunk_keys_to_save

    def merge_openie_results(self,
                             all_openie_info: List[dict],
                             chunks_to_save: Dict[str, dict],
                             ner_results_dict: Dict[str, NerRawOutput],
                             proposition_results_dict: Dict[str, PropositionRawOutput] = None) -> List[dict]:
        """
        Merges OpenIE extraction results with corresponding passage and metadata.

        This function integrates the OpenIE extraction results, including propositions and 
        entities, with their respective text passages
        using the provided chunk keys. The resulting merged data is appended to
        the `all_openie_info` list containing dictionaries with combined and organized
        data for further processing or storage.

        Parameters:
            all_openie_info (List[dict]): A list to hold dictionaries of merged OpenIE
                results and metadata for all chunks.
            chunks_to_save (Dict[str, dict]): A dict of chunk identifiers (keys) to process
                and merge OpenIE results to dictionaries with `hash_id` and `content` keys.
            ner_results_dict (Dict[str, NerRawOutput]): A dictionary mapping chunk keys
                to their corresponding NER extraction results.
            proposition_results_dict (Dict[str, PropositionRawOutput], optional): A dictionary 
                mapping chunk keys to their corresponding proposition extraction results.

        Returns:
            List[dict]: The `all_openie_info` list containing dictionaries with merged
            OpenIE results, metadata, and the passage content for each chunk.

        """

        for chunk_key, row in chunks_to_save.items():
            passage = row['content']
            chunk_openie_info = {
                'idx': chunk_key, 
                'passage': passage,
                'extracted_entities': ner_results_dict[chunk_key].unique_entities,
            }
            
            chunk_openie_info['extracted_triples'] = []
            
            # Add propositions if available
            if proposition_results_dict and chunk_key in proposition_results_dict:
                chunk_openie_info['propositions'] = proposition_results_dict[chunk_key].propositions
                
            all_openie_info.append(chunk_openie_info)

        return all_openie_info

    def save_openie_results(self, all_openie_info: List[dict]):
        """
        Computes statistics on extracted entities from OpenIE results and saves the aggregated data in a
        JSON file. The function calculates the average character and word lengths of the extracted entities
        and writes them along with the provided OpenIE information to a file.

        Parameters:
            all_openie_info : List[dict]
                List of dictionaries, where each dictionary represents information from OpenIE, including
                extracted entities.
        """
        for chunk in all_openie_info:
            for i, e in enumerate(chunk['extracted_entities']):
                if not isinstance(e, str):
                    print(chunk)
                    print(e)
                chunk['extracted_entities'][i] = str(e)
        sum_phrase_chars = sum([len(e) for chunk in all_openie_info for e in chunk['extracted_entities']])
        sum_phrase_words = sum([len(e.split()) for chunk in all_openie_info for e in chunk['extracted_entities']])
        num_phrases = sum([len(chunk['extracted_entities']) for chunk in all_openie_info])

        if len(all_openie_info) > 0:
            openie_dict = {'docs': all_openie_info, 'avg_ent_chars': round(sum_phrase_chars / num_phrases, 4),
                           'avg_ent_words': round(sum_phrase_words / num_phrases, 4)}
            with open(self.openie_results_path, 'w') as f:
                json.dump(openie_dict, f)
            logger.info(f"OpenIE results saved to {self.openie_results_path}")

    def augment_graph(self):
        """
        Provides utility functions to augment a graph by adding new nodes and edges.
        It ensures that the graph structure is extended to include additional components,
        and logs the completion status along with printing the updated graph information.
        """

        self.add_new_nodes()
        self.add_new_edges()

        logger.info(f"Graph construction completed!")
        print(self.get_graph_info())

    def add_new_nodes(self):
        """
        Adds new nodes to the graph from entity and passage embedding stores based on their attributes.

        This method identifies and adds new nodes to the graph by comparing existing nodes
        in the graph and nodes retrieved from the entity embedding store and the passage
        embedding store. The method checks attributes and ensures no duplicates are added.
        New nodes are prepared and added in bulk to optimize graph updates.
        """

        existing_nodes = {v["name"]: v for v in self.graph.vs if "name" in v.attributes()}

        entity_nodes = self.entity_embedding_store.get_text_for_all_rows()
        passage_nodes = self.chunk_embedding_store.get_text_for_all_rows()

        # proposition_nodes = self.proposition_embedding_store.get_text_for_all_rows()

        nodes = entity_nodes
        nodes.update(passage_nodes)

        # nodes.update(proposition_nodes)

        new_nodes = {}
        for node_id, node in nodes.items():
            node['name'] = node_id
            if node_id not in existing_nodes:
                for k, v in node.items():
                    if k not in new_nodes:
                        new_nodes[k] = []
                    new_nodes[k].append(v)

        if len(new_nodes) > 0:
            self.graph.add_vertices(n=len(next(iter(new_nodes.values()))), attributes=new_nodes)

    def add_new_edges(self):
        """
        Processes edges from `node_to_node_stats` to add them into a graph object while
        managing adjacency lists, validating edges, and logging invalid edge cases.
        """

        graph_adj_list = defaultdict(dict)
        graph_inverse_adj_list = defaultdict(dict)
        edge_source_node_keys = []
        edge_target_node_keys = []
        edge_metadata = []
        for edge, weight in self.node_to_node_stats.items():
            if edge[0] == edge[1]: continue
            graph_adj_list[edge[0]][edge[1]] = weight
            graph_inverse_adj_list[edge[1]][edge[0]] = weight

            edge_source_node_keys.append(edge[0])
            edge_target_node_keys.append(edge[1])
            edge_metadata.append({
                "weight": weight
            })

        valid_edges, valid_weights = [], {"weight": []}
        current_node_ids = set(self.graph.vs["name"])
        for source_node_id, target_node_id, edge_d in zip(edge_source_node_keys, edge_target_node_keys, edge_metadata):
            if source_node_id in current_node_ids and target_node_id in current_node_ids:
                valid_edges.append((source_node_id, target_node_id))
                weight = edge_d.get("weight", 1.0)
                valid_weights["weight"].append(weight)
            else:
                logger.warning(f"Edge {source_node_id} -> {target_node_id} is not valid.")
        self.graph.add_edges(
            valid_edges,
            attributes=valid_weights
        )

    def save_igraph(self):
        logger.info(
            f"Writing graph with {len(self.graph.vs())} nodes, {len(self.graph.es())} edges"
        )
        self.graph.write_graphml(self._graphml_xml_file)
        logger.info(f"Saving graph completed!")

    def get_graph_info(self) -> Dict:
        """
        Obtains detailed information about the graph such as the number of nodes,
        triples, and their classifications.

        This method calculates various statistics about the graph based on the
        stores and node-to-node relationships, including counts of phrase and
        passage nodes, total nodes, extracted propositions, triples involving passage
        nodes, synonymy triples, and total triples.

        Returns:
            Dict
                A dictionary containing the following keys and their respective values:
                - num_phrase_nodes: The number of unique phrase nodes.
                - num_passage_nodes: The number of unique passage nodes.
                - num_total_nodes: The total number of nodes (sum of phrase and passage nodes).
                - num_extracted_propositions: The number of unique extracted propositions.
                - num_triples_with_passage_node: The number of triples involving at least one
                  passage node.
                - num_synonymy_triples: The number of synonymy triples.
                - num_total_triples: The total number of triples.
        """
        graph_info = {}

        # get # of phrase nodes
        phrase_nodes_keys = self.entity_embedding_store.get_all_ids()
        graph_info["num_phrase_nodes"] = len(set(phrase_nodes_keys))

        # get # of passage nodes
        passage_nodes_keys = self.chunk_embedding_store.get_all_ids()
        graph_info["num_passage_nodes"] = len(set(passage_nodes_keys))

        # get # of total nodes
        graph_info["num_total_nodes"] = graph_info["num_phrase_nodes"] + graph_info["num_passage_nodes"]

        # get # of extracted proposiions
        graph_info["num_extracted_propositions"] = len(self.proposition_embedding_store.get_all_ids())

        num_triples_with_passage_node = 0
        passage_nodes_set = set(passage_nodes_keys)
        num_triples_with_passage_node = sum(
            1 for node_pair in self.node_to_node_stats
            if node_pair[0] in passage_nodes_set or node_pair[1] in passage_nodes_set
        )
        graph_info['num_triples_with_passage_node'] = num_triples_with_passage_node

        graph_info['num_synonymy_triples'] = len(self.node_to_node_stats) - graph_info[
            "num_extracted_propositions"] - num_triples_with_passage_node

        # get # of total triples
        graph_info["num_total_triples"] = len(self.node_to_node_stats)

        return graph_info

    def add_proposition_edges_with_entity_connections(self):
        """
        Add proposition edges to the graph where all entities within the same proposition
        are fully connected to each other.
        
        This creates:
        1. Connections between propositions and their entities
        2. Direct connections between all entities within the same proposition (fully connected)
        
        Note: This method only collects relationships in node_to_node_stats.
        Actual vertices and edges are added later by augment_graph().
        """
        if "name" in self.graph.vs:
            current_graph_nodes = set(self.graph.vs["name"])
        else:
            current_graph_nodes = set()
            
        logger.info(f"Adding proposition entities to graph with entity-entity connections")
        
        # Track which entities belong to which chunks for proper counting
        entity_to_chunks = defaultdict(set)
        
        # First pass - map entities to chunks
        for chunk_item in self.openie_info:
            chunk_id = chunk_item['idx']
            if 'propositions' in chunk_item:
                for prop in chunk_item['propositions']:
                    if 'entities' in prop:
                        for entity_text in prop['entities']:
                            entity_key = compute_mdhash_id(entity_text, prefix="entity-")
                            entity_to_chunks[entity_key].add(chunk_id)
        
        # Process each proposition and collect node-to-node statistics
        for prop_key, entities in tqdm(self.proposition_to_entities_map.items(), desc="Adding proposition edges"):
            if prop_key not in current_graph_nodes:
                # entities_in_proposition = set()
                entity_keys = []
                
                # Process all entities
                for entity_text in entities:
                    # Generate entity key 
                    entity_key = compute_mdhash_id(entity_text, prefix="entity-")
                    entity_keys.append(entity_key)

                # for entity_key in entity_keys:
                #     self.node_to_node_stats[(prop_key, entity_key)] = 1.0
                #     self.node_to_node_stats[(entity_key, prop_key)] = 1.0
                
                # Fully connect all entities within the same proposition (create a clique)
                for i in range(len(entity_keys)):
                    for j in range(i+1, len(entity_keys)):
                        entity1 = entity_keys[i]
                        entity2 = entity_keys[j]
                        
                        # Add edges between all entity pairs (bidirectionally)
                        self.node_to_node_stats[(entity1, entity2)] = self.node_to_node_stats.get(
                            (entity1, entity2), 0.0) + 1
                        self.node_to_node_stats[(entity2, entity1)] = self.node_to_node_stats.get(
                            (entity2, entity1), 0.0) + 1

        
        # Update entity to chunk counts based on the mapping we built
        for entity_key, chunk_set in entity_to_chunks.items():
            self.ent_node_to_num_chunk[entity_key] = self.ent_node_to_num_chunk.get(entity_key, 0) + len(chunk_set)
                
        logger.info(f"Finished adding proposition edges with fully connected entity nodes")
        
    def prepare_retrieval_objects(self):
        """
        Prepares various in-memory objects and attributes necessary for fast retrieval processes, such as embedding data and graph relationships, ensuring consistency
        and alignment with the underlying graph structure.
        """

        logger.info("Preparing for fast retrieval.")

        logger.info("Loading keys.")
        self.query_to_embedding: Dict = {'proposition': {}, 'passage': {}}

        self.entity_node_keys: List = list(self.entity_embedding_store.get_all_ids()) # a list of phrase node keys
        self.passage_node_keys: List = list(self.chunk_embedding_store.get_all_ids()) # a list of passage node keys
        self.proposition_node_keys: List = list(self.proposition_embedding_store.get_all_ids()) # a list of proposition/fact node keys

        # If using propositions, we need to handle the graph structure differently
        if hasattr(self, 'proposition_to_entities_map'):
            # For proposition-based KG, the graph structure is more complex
            # Number of nodes could be different based on how many proposition/entity nodes were added
            # So just log the vertex count instead of asserting
            logger.info(f"Graph has {self.graph.vcount()} vertices and {self.graph.ecount()} edges")
        else:
            # For the standard KG structure
            expected_nodes = len(self.entity_node_keys) + len(self.passage_node_keys)
            if self.graph.vcount() != expected_nodes:
                logger.warning(f"Graph has {self.graph.vcount()} nodes, expected {expected_nodes} nodes")
            else:
                logger.info(f"Graph structure validated with {expected_nodes} nodes")

        igraph_name_to_idx = {node["name"]: idx for idx, node in enumerate(self.graph.vs)} # from node key to the index in the backbone graph
        self.node_name_to_vertex_idx = igraph_name_to_idx
        
        # Map node keys to vertex indices
        self.entity_node_idxs = []
        for node_key in self.entity_node_keys:
            if node_key in igraph_name_to_idx:
                self.entity_node_idxs.append(igraph_name_to_idx[node_key])
        
        self.passage_node_idxs = []
        self.passage_node_key_to_idx = {}
        for i, node_key in enumerate(self.passage_node_keys):
            if node_key in igraph_name_to_idx:
                self.passage_node_idxs.append(igraph_name_to_idx[node_key])
            self.passage_node_key_to_idx[node_key] = i
            
        
        # Map proposition/fact nodes if they exist in the graph
        self.proposition_node_idxs = []
        for node_key in self.proposition_node_keys:
            if node_key in igraph_name_to_idx:
                self.proposition_node_idxs.append(igraph_name_to_idx[node_key])

        logger.info("Loading embeddings.")
        self.entity_embeddings = np.array(self.entity_embedding_store.get_embeddings(self.entity_node_keys))
        self.passage_embeddings = np.array(self.chunk_embedding_store.get_embeddings(self.passage_node_keys))


        self.proposition_embeddings = np.array(self.proposition_embedding_store.get_embeddings(self.proposition_node_keys))

        self.prop_key_to_propositions = {prop_key: self.proposition_embeddings[i] for i, prop_key in enumerate(self.proposition_node_keys)}
        # cache all proposition embeddings to speed up scoring
        self.all_proposition_embeddings = np.array([self.prop_key_to_propositions[prop_key] for prop_key in self.proposition_embedding_store.get_all_ids()])

        if hasattr(self, 'proposition_to_entities_map'):
            # Setting path length and beam_width to the following values for the first-stage PPR.
            self.beam_search = BeamSearchPathFinder(
                self, 
                beam_width=200,
                max_path_length=1,
                embedding_combination="average"
            )
        self.query_fact_scores = {}

        self.ready_to_retrieve = True

    def get_query_embeddings(self, queries: List[str] | List[QuerySolution]):
        """
        Retrieves embeddings for given queries and updates the internal query-to-embedding mapping. The method determines whether each query
        is already present in the `self.query_to_embedding` dictionary under the keys 'proposition' and 'passage'. If a query is not present in
        either, it is encoded into embeddings using the embedding model and stored.

        Args:
            queries List[str] | List[QuerySolution]: A list of query strings or QuerySolution objects. Each query is checked for
            its presence in the query-to-embedding mappings.
        """

        all_query_strings = []
        for query in queries:
            if isinstance(query, QuerySolution) and (
                    query.question not in self.query_to_embedding['proposition'] or query.question not in
                    self.query_to_embedding['passage']):
                all_query_strings.append(query.question)
            elif query not in self.query_to_embedding['proposition'] or query not in self.query_to_embedding['passage']:
                all_query_strings.append(query)

        if len(all_query_strings) > 0:
            # get all query embeddings
            logger.info(f"Encoding {len(all_query_strings)} queries for query_to_passage.")
            query_embeddings_for_proposition = self.embedding_model.batch_encode(all_query_strings,
                                                                            instruction=get_query_instruction('query_to_passage'),
                                                                            norm=True)

            for query, embedding in zip(all_query_strings, query_embeddings_for_proposition):
                self.query_to_embedding['proposition'][query] = embedding

            logger.info(f"Encoding {len(all_query_strings)} queries for query_to_passage.")
            query_embeddings_for_passage = self.embedding_model.batch_encode(all_query_strings,
                                                                             instruction=get_query_instruction('query_to_passage'),
                                                                             norm=True)
            for query, embedding in zip(all_query_strings, query_embeddings_for_passage):
                self.query_to_embedding['passage'][query] = embedding

    def dense_passage_retrieval(self, query: str, no_sort: bool = False) -> Tuple[np.ndarray, np.ndarray]:
        """
        Conduct dense passage retrieval to find relevant documents for a query.

        This function processes a given query using a pre-trained embedding model
        to generate query embeddings. The similarity scores between the query
        embedding and passage embeddings are computed using dot product, followed
        by score normalization. Finally, the function ranks the documents based
        on their similarity scores and returns the ranked document identifiers
        and their scores.

        If propositions are enabled, this function will calculate document scores
        as the maximum score of any proposition belonging to the document, rather
        than using the score of the entire passage.

        Parameters
        ----------
        query : str
            The input query for which relevant passages should be retrieved.

        Returns
        -------
        tuple : Tuple[np.ndarray, np.ndarray]
            A tuple containing two elements:
            - A list of sorted document identifiers based on their relevance scores.
            - A numpy array of the normalized similarity scores for the corresponding
              documents.
        """
        if query in self.query_fact_scores:
            return self.query_fact_scores[query]
        query_embedding = self.query_to_embedding['passage'].get(query, None)
        if query_embedding is None:
            query_embedding = self.embedding_model.batch_encode(query,
                                                                instruction=get_query_instruction('query_to_passage'),
                                                                norm=True)
        
        # Standard dense passage retrieval if propositions not enabled
        query_doc_scores = np.dot(self.passage_embeddings, query_embedding.T)
        
        query_doc_scores = np.squeeze(query_doc_scores) if query_doc_scores.ndim == 2 else query_doc_scores

        # Normalize scores
        query_doc_scores = min_max_normalize(query_doc_scores)

        if no_sort:
            self.query_fact_scores[query] = (np.arange(len(query_doc_scores)), query_doc_scores)
            return self.query_fact_scores[query]

        sorted_doc_ids = np.argsort(query_doc_scores)[::-1]
        sorted_doc_scores = query_doc_scores[sorted_doc_ids.tolist()]
        return sorted_doc_ids, sorted_doc_scores


    def get_top_k_weights(self,
                          link_top_k: int,
                          all_phrase_weights: np.ndarray,
                          linking_score_map: Dict[str, float]) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        This function filters the all_phrase_weights to retain only the weights for the
        top-ranked phrases in terms of the linking_score_map. It also filters linking scores
        to retain only the top `link_top_k` ranked nodes. Non-selected phrases in phrase
        weights are reset to a weight of 0.0.

        Args:
            link_top_k (int): Number of top-ranked nodes to retain in the linking score map.
            all_phrase_weights (np.ndarray): An array representing the phrase weights, indexed
                by phrase ID.
            linking_score_map (Dict[str, float]): A mapping of phrase content to its linking
                score, sorted in descending order of scores.

        Returns:
            Tuple[np.ndarray, Dict[str, float]]: A tuple containing the filtered array
            of all_phrase_weights with unselected weights set to 0.0, and the filtered
            linking_score_map containing only the top `link_top_k` phrases.
        """        
        # Sort and limit phrases by score
        sorted_phrases = sorted(linking_score_map.items(), key=lambda x: x[1], reverse=True)
        top_phrases = sorted_phrases[:link_top_k]
        linking_score_map = dict(top_phrases)
        
        # Convert phrase text to entity keys
        top_k_phrase_keys = set(linking_score_map.keys())
        # Filter weights to only include top phrases
        entity_keys_in_graph = set()
        for phrase_key in self.node_name_to_vertex_idx:
            if phrase_key not in top_k_phrase_keys:
                phrase_id = self.node_name_to_vertex_idx.get(phrase_key, None)
                if phrase_id is not None:
                    all_phrase_weights[phrase_id] = 0.0
            else:
                entity_keys_in_graph.add(phrase_key)
        
        # Check which phrase keys were not found in the graph
        missing_keys = top_k_phrase_keys - entity_keys_in_graph
        if missing_keys:
            logger.warning(f"{len(missing_keys)} top phrase keys not found in graph")
            logger.warning(f"Missing keys: {missing_keys}, top k phrases: {top_k_phrases}, entity keys in graph: {entity_keys_in_graph}")
        
        # Update linking_score_map to only include phrases that exist in the graph
        filtered_linking_score_map = {}
        for phrase_key, score in linking_score_map.items():
            if phrase_key in entity_keys_in_graph:
                filtered_linking_score_map[phrase_key] = score
        
        # Replace the original map with filtered version
        linking_score_map = filtered_linking_score_map
        
        # Check if counts match, log warning instead of failing
        if np.count_nonzero(all_phrase_weights) != len(linking_score_map):
            logger.warning(f"Weight count mismatch: {np.count_nonzero(all_phrase_weights)} weights vs {len(linking_score_map)} phrases")
            
            # Fix the issue by adjusting the weights to match the actual entities in the graph
            # This prevents the assertion error without breaking the functionality
            available_weights = np.count_nonzero(all_phrase_weights)
            if available_weights > 0:
                logger.info(f"Continuing with {available_weights} available weights")
            else:
                logger.warning("No weights available, will use empty mapping")
        
        return all_phrase_weights, linking_score_map

    def retrieve_grid_search(
        self,
        queries: List[str],
        gold_docs: List[List[str]],
        select_top_k_paths_range: List[int] = [5, 10, 15, 20, 30, 40, 50],
        select_top_k_entities_range: List[int] = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100],
        ppr_damping_factor_range: List[float] = [0.5],
        k: int = 5,
        beam_size: int = 3
    ) -> Tuple[Dict, Dict]:
        """
        Performs grid search to optimize parameters for graph_search_with_proposition_entities_single_stage.
        
        Args:
            queries: List of query strings
            gold_docs: List of lists containing gold standard document IDs for each query
            select_top_k_paths_range: Range of values to try for select_top_k_paths
            select_top_k_entities_range: Range of values to try for select_top_k_entities
            ppr_damping_factor_range: Range of values to try for ppr_damping_factor
            k: The k value for Recall@k metric (default: 100)
            
        Returns:
            Tuple containing:
            1. Dictionary with best parameters found
            2. Dictionary with Recall@k scores for all parameter combinations
        """
        # Initialize evaluation metric
        logger.info("Starting beam search grid optimization")
        logger.info(f"Parameter ranges:")
        logger.info(f"  select_top_k_paths: {select_top_k_paths_range}")
        logger.info(f"  select_top_k_entities: {select_top_k_entities_range}")
        logger.info(f"  ppr_damping_factor: {ppr_damping_factor_range}")
        logger.info(f"Beam size: {beam_size}")
        if not self.ready_to_retrieve:
            self.prepare_retrieval_objects()
        self.get_query_embeddings(queries)
        retrieval_recall_evaluator = RetrievalRecall(global_config=self.global_config)
        
        # Store results for all combinations
        try:
            with open(f"grid_search_results.json") as inp:
                all_results = json.load(inp)
        except FileNotFoundError:
            all_results = {}
        
        # Start with default parameters
        initial_params = {
            'select_top_k_paths': 20,
            'select_top_k_entities': 40,
            'ppr_damping_factor': 0.5
        }
        best_params = initial_params.copy()
        best_recall = 0.0
        
        def evaluate_params(params: Dict) -> float:
            """Helper function to evaluate a parameter combination"""
            param_key = f"paths_{params['select_top_k_paths']}_entities_{params['select_top_k_entities']}_damping_{params['ppr_damping_factor']}"
            
            if param_key in all_results:
                return all_results[param_key][f"Recall@{k}"]
            num_to_retrieve = 200
            logger.info(f"Evaluating parameters: {params}")
            retrieval_results = []
            for query in tqdm(queries):
                doc_ids, doc_scores, _, _, _ = self.graph_search_with_proposition_entities_single_stage(
                    query=query,
                    select_top_k_paths=params['select_top_k_paths'],
                    select_top_k_entities=params['select_top_k_entities'],
                    ppr_damping_factor=params['ppr_damping_factor']
                )

                top_k_docs = [self.chunk_embedding_store.get_row(self.passage_node_keys[idx])["content"] for idx in doc_ids[:num_to_retrieve]]
                retrieval_results.append(QuerySolution(question=query, docs=top_k_docs, doc_scores=doc_scores[:num_to_retrieve]))

            k_list = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]
            overall_retrieval_result, example_retrieval_results = retrieval_recall_evaluator.calculate_metric_scores(gold_docs=gold_docs, retrieved_docs=[retrieval_result.docs for retrieval_result in retrieval_results], k_list=k_list)
            logger.info(f"Evaluation results for retrieval: {overall_retrieval_result}")
            all_results[param_key] = overall_retrieval_result
            with open(f"grid_search_results.json", "w") as f:
                json.dump(all_results, f)
            return all_results[param_key][f"Recall@{k}"]

        def get_neighbors(params: Dict, param_name: str, param_range: List) -> List[Dict]:
            """Generate neighboring parameter combinations by varying one parameter"""
            current_idx = param_range.index(params[param_name])
            right_neighbors = []
            left_neighbors = []
            
            # Try increasing values
            for i in range(current_idx + 1, len(param_range)):
                neighbor = params.copy()
                neighbor[param_name] = param_range[i]
                right_neighbors.append(neighbor)
                
            # Try decreasing values
            for i in range(current_idx - 1, -1, -1):
                neighbor = params.copy()
                neighbor[param_name] = param_range[i]
                left_neighbors.append(neighbor)
                
            return right_neighbors, left_neighbors

            # Evaluate initial parameters
        best_params = initial_params.copy()
        beam = [(best_params, evaluate_params(best_params))]
        while True:
            last_best_param = best_params.copy()
            # beam = [(best_params, evaluate_params(best_params))]
            
            # Search for optimal parameters
            for param_name, param_range in [
                ('select_top_k_paths', select_top_k_paths_range),
                ('select_top_k_entities', select_top_k_entities_range),
                ('ppr_damping_factor', ppr_damping_factor_range)
            ]:
                logger.info(f"\nOptimizing parameter: {param_name}")
                logger.info(f"Current beam:")
                for params, score in beam:
                    logger.info(f"  {params} -> Recall@{k}: {score:.4f}")
                # Sort range to ensure we can search in both directions
                param_range = sorted(param_range)
                
            # Generate and evaluate neighbors for each parameter combination in the beam
                new_beam = []
                for params, score in beam:
                    right_neighbors, left_neighbors = get_neighbors(params, param_name, param_range)
                    best_score = 0.0
                    for neighbor in right_neighbors:
                        if param_name == 'select_top_k_entities' and neighbor['select_top_k_entities'] > 4 * neighbor['select_top_k_paths']:
                            break

                        neighbor_score = evaluate_params(neighbor)
                        logger.info(f"  Neighbor: {neighbor} -> Recall@{k}: {neighbor_score:.4f}")
                        new_beam.append((neighbor, neighbor_score))
                        if neighbor_score > best_score:
                            best_score = neighbor_score
                        else:
                            break
                    best_score = 0.0
                    for neighbor in left_neighbors:
                        neighbor_score = evaluate_params(neighbor)
                        logger.info(f"  Neighbor: {neighbor} -> Recall@{k}: {neighbor_score:.4f}")
                        new_beam.append((neighbor, neighbor_score))
                        if neighbor_score > best_score:
                            best_score = neighbor_score
                        else:
                            break
                
                # Add current beam to new beam and select top beam_size combinations
                new_beam.extend(beam)
                new_beam.sort(key=lambda x: x[1], reverse=True)
                beam = new_beam[:beam_size]

                logger.info(f"Top {beam_size} combinations after optimizing {param_name}:")
                for params, score in beam:
                    logger.info(f"  {params} -> Recall@{k}: {score:.4f}")

            # Get best parameters and their score
            best_params, best_recall = beam[0]
            # best_params['recall@k'] = best_recall

            logger.info("\nCurrent grid search stage completed!")
            logger.info(f"Best parameters found: {best_params}")
            logger.info(f"Best Recall@{k}: {best_recall:.4f}")

            if best_params['select_top_k_paths'] == last_best_param['select_top_k_paths'] and best_params['select_top_k_entities'] == last_best_param['select_top_k_entities'] and best_params['ppr_damping_factor'] == last_best_param['ppr_damping_factor']:
                logger.info("No improvement, breaking")
                break
            else:
                logger.info("Improvement, continuing to next stage")
        
        return best_params, all_results

    def graph_search_with_proposition_entities_single_stage(self, query: str,
                                        select_top_k_paths = 10,
                                        select_top_k_entities = 30,
                                        ppr_damping_factor = 0.75) -> Tuple[np.ndarray, np.ndarray]:
        """
        Computes document scores based on fact/proposition-based similarity and relevance using personalized
        PageRank (PPR) and dense retrieval models. This function combines the signal from the relevant
        facts/propositions identified with passage similarity and graph-based search for enhanced result ranking.

        With focused graph approach, this runs a first iteration with the full graph and beam path length 1,
        then a second iteration with a subgraph of the top documents and beam path length 2.

        Parameters:
            query (str): The input query string for which similarity and relevance computations
                need to be performed.
        Returns:
            Tuple[np.ndarray, np.ndarray]: A tuple containing two arrays:
                - The first array corresponds to document IDs sorted based on their scores.
                - The second array consists of the PPR scores associated with the sorted document IDs.
        """
        passage_node_weight = 0.05
        # Get passage scores from dense retrieval (will be reused if we do a second iteration)
        dpr_sorted_doc_ids, dpr_sorted_doc_scores = self.dense_passage_retrieval(query, no_sort=True)
        # First iteration - standard approach
        linking_score_map = {}  # from phrase to the average scores of the facts that contain the phrase
        phrase_scores = {}  # store all fact scores for each phrase
        phrase_weights = np.zeros(len(self.graph.vs['name']))
        passage_weights = np.zeros(len(self.graph.vs['name']))

        passage_weights = np.where(np.isnan(passage_weights), 0, passage_weights)

        # Find the top proposition paths
        paths, other_candidates, initial_paths = self.beam_search.find_paths(query)
        paths = paths[:select_top_k_paths]

        # Process entity scores from paths
        top_entities_and_scores = self.beam_search.get_entities_from_paths(paths)[:select_top_k_entities]
        for entity_key, scores in top_entities_and_scores:
            phrase_id = self.node_name_to_vertex_idx[entity_key]
            phrase_weights[phrase_id] = 1.0
            phrase_scores[entity_key] = np.sum(scores)
        
        phrase_weights_max = np.max(phrase_weights)
        phrase_weights_min = np.min(phrase_weights)
        phrase_weights = min_max_normalize(phrase_weights)


        for i, doc_id in enumerate(dpr_sorted_doc_ids):
            passage_node_key = self.passage_node_keys[doc_id]
            passage_dpr_score = dpr_sorted_doc_scores[i]
            passage_node_id = self.node_name_to_vertex_idx[passage_node_key]
            passage_weights[passage_node_id] = passage_dpr_score * passage_node_weight
        
        # Combine weights
        node_weights = phrase_weights + passage_weights


        # Ensure we have valid weights
        try:
            assert len(top_entities_and_scores) > 0, f'No phrases found in the graph for the given query: {query}'
        except AssertionError as e:
            logger.warning(f"No phrases found in the graph for the given query: {query}")
            logger.warning(f"Query embedding: {self.query_to_embedding['passage'][query]}")
            logger.warning(f"dpr doc scores: {dpr_sorted_doc_scores}")
            logger.warning(f"Top entities and scores: {top_entities_and_scores}")
            logger.warning(f"Paths: {paths}")
            logger.warning(f"Other candidates: {other_candidates}")
            logger.warning(f"max phrase_weights: {phrase_weights_max}")
            logger.warning(f"min phrase_weights: {phrase_weights_min}")
            logger.warning(f"max doc_scores: {np.max(dpr_sorted_doc_scores)}")
            logger.warning(f"min doc_scores: {np.min(dpr_sorted_doc_scores)}")
            logger.warning(f"sum node_weights: {sum(node_weights)}")
            logger.warning(f"top entities and scores: {top_entities_and_scores}")
            raise e

        first_ppr_doc_ids, first_ppr_doc_scores, unsorted_first_ppr_doc_scores = self.run_ppr(node_weights, damping=ppr_damping_factor)
        
        # If not using focused approach, return first iteration results
        return first_ppr_doc_ids, first_ppr_doc_scores, paths[:5], top_entities_and_scores, other_candidates

        
    def graph_search_with_proposition_entities(self, query: str,
                                        link_top_k: int,
                                        passage_node_weight: float = 0.05,
                                        use_focused_graph: bool = False,
                                        focus_top_k: int = 200) -> Tuple[np.ndarray, np.ndarray]:
        """
        Computes document scores based on fact/proposition-based similarity and relevance using personalized
        PageRank (PPR) and dense retrieval models. This function combines the signal from the relevant
        facts/propositions identified with passage similarity and graph-based search for enhanced result ranking.

        With focused graph approach, this runs a first iteration with the full graph and beam path length 1,
        then a second iteration with a subgraph of the top documents and beam path length 2.

        Parameters:
            query (str): The input query string for which similarity and relevance computations
                need to be performed.
            link_top_k (int): The number of top phrases to include from the linking score map for
                downstream processing.
            passage_node_weight (float): Default weight to scale passage scores in the graph.
            use_focused_graph (bool): Whether to use the two-iteration approach with a focused subgraph.
            focus_top_k (int): Number of top documents to include in the focused subgraph (default: 200).

        Returns:
            Tuple[np.ndarray, np.ndarray]: A tuple containing two arrays:
                - The first array corresponds to document IDs sorted based on their scores.
                - The second array consists of the PPR scores associated with the sorted document IDs.
        """
        # Get passage scores from dense retrieval (will be reused if we do a second iteration)
        dpr_sorted_doc_ids, dpr_sorted_doc_scores = self.dense_passage_retrieval(query, no_sort=True)
        
        # First iteration - standard approach
        linking_score_map = {}  # from phrase to the average scores of the facts that contain the phrase
        phrase_scores = {}  # store all fact scores for each phrase
        phrase_weights = np.zeros(len(self.graph.vs['name']))
        passage_weights = np.zeros(len(self.graph.vs['name']))

        passage_weights = np.where(np.isnan(passage_weights), 0, passage_weights)

        # Find the top proposition paths
        paths, other_candidates, initial_paths = self.beam_search.find_paths(query)
        paths = paths[:20] # k=50

        # Process entity scores from paths
        top_entities_and_scores = self.beam_search.get_entities_from_paths(paths)[:40] # k = 50
        for entity_key, scores in top_entities_and_scores:
            phrase_id = self.node_name_to_vertex_idx[entity_key]
            phrase_weights[phrase_id] = 1.0
            phrase_scores[entity_key] = np.sum(scores)
        
        phrase_weights_max = np.max(phrase_weights)
        phrase_weights_min = np.min(phrase_weights)
        phrase_weights = min_max_normalize(phrase_weights)

        # Apply passage scores
        for i, doc_id in enumerate(dpr_sorted_doc_ids):
            passage_node_key = self.passage_node_keys[doc_id]
            passage_dpr_score = dpr_sorted_doc_scores[i]
            passage_node_id = self.node_name_to_vertex_idx[passage_node_key]
            passage_weights[passage_node_id] = passage_dpr_score * passage_node_weight
        
        # Combine weights
        node_weights = phrase_weights + passage_weights

        # Ensure we have valid weights
        try:
            assert len(top_entities_and_scores) > 0, f'No phrases found in the graph for the given query: {query}'
        except AssertionError as e:
            logger.warning(f"No phrases found in the graph for the given query: {query}")
            logger.warning(f"Query embedding: {self.query_to_embedding['passage'][query]}")
            logger.warning(f"dpr doc scores: {dpr_sorted_doc_scores}")
            logger.warning(f"Top entities and scores: {top_entities_and_scores}")
            logger.warning(f"Paths: {paths}")
            logger.warning(f"Other candidates: {other_candidates}")
            logger.warning(f"max phrase_weights: {phrase_weights_max}")
            logger.warning(f"min phrase_weights: {phrase_weights_min}")
            logger.warning(f"max doc_scores: {np.max(dpr_sorted_doc_scores)}")
            logger.warning(f"min doc_scores: {np.min(dpr_sorted_doc_scores)}")
            logger.warning(f"sum node_weights: {sum(node_weights)}")
            logger.warning(f"top entities and scores: {top_entities_and_scores}")
            raise e

        # Run PPR for first iteration
        first_ppr_doc_ids, first_ppr_doc_scores, unsorted_first_ppr_doc_scores = self.run_ppr(node_weights, damping=0.75)
        
        # If not using focused approach, return first iteration results
        # use_focused_graph = False
        # if not use_focused_graph:
        #     return first_ppr_doc_ids, first_ppr_doc_scores, paths[:5], top_entities_and_scores, other_candidates
        
        # Second iteration - focused subgraph with longer paths
        # logger.info(f"Running second iteration with focused subgraph from top {focus_top_k} documents")
        
        # Create mapping from passage keys to their position in passage_node_keys (for fast lookups)
        passage_key_to_idx = {key: idx for idx, key in enumerate(self.passage_node_keys)}
        
        focus_top_k = 50
        # Get top-k documents to focus on
        top_doc_keys = [self.passage_node_keys[doc_id] for doc_id in first_ppr_doc_ids[:focus_top_k]]
        top_propositions = flatten_propositions([self.chunk_propositions[doc_key] for doc_key in top_doc_keys])
        top_proposition_keys = [compute_mdhash_id(prop['text'], prefix="proposition-") for prop in top_propositions]
        

        top_doc_vertices = [self.node_name_to_vertex_idx[key] for key in top_doc_keys]
        
        # Create sets to efficiently track vertices
        top_doc_vertex_set = set(top_doc_vertices)
        vertices_to_include = set(top_doc_vertices)
        
        # Add connected entity vertices
        entity_vertices = set()
        for doc_vertex in top_doc_vertices:
            neighbors = self.graph.neighbors(doc_vertex, mode="all")
            for neighbor in neighbors:
                if self.graph.vs[neighbor]["name"].startswith("entity-"):
                    entity_vertices.add(neighbor)
                    vertices_to_include.add(neighbor)
        
        # Create subgraph efficiently using igraph's induced_subgraph
        subgraph = self.graph.induced_subgraph(list(vertices_to_include))
        
        subgraph_node_name_to_vertex_idx = {node["name"]: idx for idx, node in enumerate(subgraph.vs)}

        # Create mapping from original vertex IDs to subgraph IDs (fast lookups)
        orig_to_sub = {self.node_name_to_vertex_idx[name]: i_sub for name, i_sub in subgraph_node_name_to_vertex_idx.items()}
        
        # Create reverse mapping from subgraph IDs to original vertex names (fast lookups)
        sub_to_name = {i: v for v, i in subgraph_node_name_to_vertex_idx.items()}
        
        # Save original beam search settings
        original_graph = self.beam_search.active_graph
        original_beam_width = self.beam_search.beam_width
        original_path_length = self.beam_search.max_path_length
        original_node_name_to_vertex_idx = self.beam_search.node_name_to_vertex_idx
        original_second_stage_filter_k = self.beam_search.second_stage_filter_k
        
        # Update beam search for second iteration
        self.beam_search.active_graph = subgraph
        # self.beam_search.beam_width = 4
        # self.beam_search.max_path_length = 3 # Increase path length for focused search
        # self.beam_search.second_stage_filter_k=40

        self.beam_search.beam_width=self.global_config.beam_width
        self.beam_search.max_path_length=self.global_config.max_path_length
        self.beam_search.second_stage_filter_k=self.global_config.second_stage_filter_k

        self.beam_search.set_node_name_to_vertex_idx(subgraph_node_name_to_vertex_idx)
        self.beam_search.clear_caches()
        
        # Run beam search on focused subgraph
        try:
            # Get new paths in the focused subgraph
            try:
                focused_paths, other_candidates, initial_paths = self.beam_search.find_paths(query, prop_set=top_proposition_keys)
                focused_paths = focused_paths[:5]
            except KeyError as e:
                logger.warning(f"Error finding paths in focused subgraph: {str(e)}")
                raise e
            
            focused_entities_and_scores = self.beam_search.get_entities_from_paths(focused_paths)[:5]
            initial_entities_and_scores = self.beam_search.get_entities_from_paths(initial_paths)[:5]
            
            
            # Create new weights for subgraph vertices
            sub_phrase_weights = np.zeros(subgraph.vcount())
            initial_sub_phrase_weights = np.zeros(subgraph.vcount())
            # intermediate_sub_phrase_weights = np.zeros(subgraph.vcount())
            # intermediate_sub_phrase_weights_2 = np.zeros(subgraph.vcount())
            sub_passage_weights = np.zeros(subgraph.vcount())
            sub_phrase_scores = np.zeros(subgraph.vcount())
            sub_phrase_scores_dict = {}
            
            # Process entity scores
            for entity_key, scores in focused_entities_and_scores:
                # Get original vertex ID and map to subgraph ID
                orig_vertex = self.node_name_to_vertex_idx[entity_key]
                try:
                    sub_vertex = orig_to_sub[orig_vertex]
                except KeyError as e:
                    logger.warning(f"Entity key {entity_key} not found in subgraph")
                    raise e
                max_score = np.max(scores)
                sub_phrase_weights[sub_vertex] = max_score

            for entity_key, scores in initial_entities_and_scores:
                 # Get original vertex ID and map to subgraph ID
                orig_vertex = self.node_name_to_vertex_idx[entity_key]
                try:
                    sub_vertex = orig_to_sub[orig_vertex]
                except KeyError as e:
                    logger.warning(f"Entity key {entity_key} not found in subgraph")
                    raise e
                max_score = np.max(scores)
                initial_sub_phrase_weights[sub_vertex] = max_score

            
            # Normalize phrase weights
            sub_phrase_weights = min_max_normalize(sub_phrase_weights) if np.sum(sub_phrase_weights) > 0 else sub_phrase_weights
            initial_sub_phrase_weights = min_max_normalize(initial_sub_phrase_weights)
            
            sub_phrase_weights = np.max([sub_phrase_weights, initial_sub_phrase_weights], axis=0)
            
            # Apply passage weights from dense retrieval (reusing scores from first iteration)
            for i, doc_id in enumerate(dpr_sorted_doc_ids):
                passage_key = self.passage_node_keys[doc_id]
                orig_vertex = self.node_name_to_vertex_idx[passage_key]
                
                # Only include if in our subgraph
                if orig_vertex in orig_to_sub:
                    sub_vertex = orig_to_sub[orig_vertex]
                    sub_passage_weights[sub_vertex] = dpr_sorted_doc_scores[i] * passage_node_weight

                
            # Combine weights for subgraph
            sub_node_weights = sub_phrase_weights + sub_passage_weights
            
            # Run PPR on subgraph
            reset_prob = np.where(np.isnan(sub_node_weights) | (sub_node_weights < 0), 0, sub_node_weights)
            sub_pagerank_scores = subgraph.personalized_pagerank(
                vertices=range(subgraph.vcount()),
                damping=0.45,
                directed=False,
                weights='weight',
                reset=reset_prob,
                implementation='prpack'
            )
            # Map PPR scores back to documents efficiently
            # First, create a new scores array initialized with lower scores for non-focused docs

            # Need unsorted first ppr doc scores here
            final_doc_scores = unsorted_first_ppr_doc_scores.copy()
            # final_doc_scores = first_ppr_doc_scores.copy()

            # Decrement scores for all documents (to ensure focused docs will rank higher)
            lowest_focused_score = min(sub_pagerank_scores)
            # print("lowest_focused_score: ", lowest_focused_score)
            # print("first_ppr_doc_scores[:2*focus_top_k]: ", first_ppr_doc_scores[:2*focus_top_k])
            non_focused_scaling = lowest_focused_score * 0.5
            final_doc_scores = final_doc_scores * non_focused_scaling
            
            # Now set scores for focused documents based on subgraph PPR results
            for sub_idx, score in enumerate(sub_pagerank_scores):
                name = sub_to_name[sub_idx]
                if name.startswith("chunk-") and name in passage_key_to_idx:
                    doc_idx = passage_key_to_idx[name]
                    final_doc_scores[doc_idx] = score
            
            # Sort final results
            final_doc_ids = np.argsort(final_doc_scores)[::-1]
            # print("final_doc_ids: ", final_doc_ids[:2*focus_top_k])
            final_doc_scores = final_doc_scores[final_doc_ids]
        finally:
            # Restore original beam search settings
            self.beam_search.active_graph = original_graph
            self.beam_search.beam_width = original_beam_width
            self.beam_search.max_path_length = original_path_length
            self.beam_search.second_stage_filter_k = original_second_stage_filter_k
            self.beam_search.set_node_name_to_vertex_idx(original_node_name_to_vertex_idx)
            self.beam_search.clear_caches()
        
        assert len(final_doc_ids) == len(self.passage_node_idxs), \
            f"Doc prob length {len(final_doc_ids)} != corpus length {len(self.passage_node_idxs)}"
        
        return final_doc_ids, final_doc_scores, focused_paths[:5], focused_entities_and_scores, other_candidates
    
    def run_ppr(self,
                reset_prob: np.ndarray,
                damping: float =0.5) -> Tuple[np.ndarray, np.ndarray]:
        """
        Runs Personalized PageRank (PPR) on a graph and computes relevance scores for
        nodes corresponding to document passages. The method utilizes a damping
        factor for teleportation during rank computation and can take a reset
        probability array to influence the starting state of the computation.

        Parameters:
            reset_prob (np.ndarray): A 1-dimensional array specifying the reset
                probability distribution for each node. The array must have a size
                equal to the number of nodes in the graph. NaNs or negative values
                within the array are replaced with zeros.
            damping (float): A scalar specifying the damping factor for the
                computation. Defaults to 0.5 if not provided or set to `None`.

        Returns:
            Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays. The
                first array represents the sorted node IDs of document passages based
                on their relevance scores in descending order. The second array
                contains the corresponding relevance scores of each document passage
                in the same order.
        """
        # time_start = time.time()
        if damping is None: damping = 0.5 # for potential compatibility
        reset_prob = np.where(np.isnan(reset_prob) | (reset_prob < 0), 0, reset_prob)
        pagerank_scores = self.graph.personalized_pagerank(
            vertices=range(len(self.node_name_to_vertex_idx)),
            damping=damping,
            directed=False,
            weights='weight',
            reset=reset_prob,
            implementation='prpack'
        )

        doc_scores = np.array([pagerank_scores[idx] for idx in self.passage_node_idxs])
        sorted_doc_ids = np.argsort(doc_scores)[::-1]
        sorted_doc_scores = doc_scores[sorted_doc_ids.tolist()]
        # time_end = time.time()
        # print(f"Time taken for PPR: {time_end - time_start:.4f} seconds")

        return sorted_doc_ids, sorted_doc_scores, doc_scores